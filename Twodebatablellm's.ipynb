{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ln-hH5vGrai5",
        "outputId": "0ca32b19-d213-4507-fdc7-f3da72fff40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debatable_statement by model1\n",
            "Climate change is the most significant issue facing the world.The U.S. Department of Agriculture (USDA) has issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the potential for a \"significant increase in the number of livestock deaths due to disease and other causes.\"\n",
            "\n",
            "The warning comes after the U.S. Department of Agriculture (USDA) issued a warning to farmers about the\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input length of input_ids is 1000, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bdb400c72926>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"debatable_statement by model1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebatable_statement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mdebatable_statement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_debatable_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebatable_statement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"debatable_statement by model2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebatable_statement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-bdb400c72926>\u001b[0m in \u001b[0;36mgenerate_debatable_statement\u001b[0;34m(initial_statement, model_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Generate a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     output = model.generate(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1497\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_cache_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_generated_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_default_max_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m         \u001b[0;31m# 7. determine generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_ids_length\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m             \u001b[0minput_ids_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decoder_input_ids\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1150\u001b[0m                 \u001b[0;34mf\"Input length of {input_ids_string} is {input_ids_length}, but `max_length` is set to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;34mf\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 1000, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "def generate_debatable_statement(initial_statement, model_name='gpt2'):\n",
        "    # Load pre-trained model tokenizer (vocabulary)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Encode the initial statement and add the EOS token\n",
        "    input_ids = tokenizer.encode(initial_statement + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # Load pre-trained model (weights)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    # Generate a response\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=1000,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Decode the response\n",
        "    debatable_statement = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return debatable_statement\n",
        "\n",
        "# Example usage\n",
        "debatable_statement = \"Climate change is the most significant issue facing the world.\"\n",
        "for i in range(10):\n",
        "  debatable_statement = generate_debatable_statement(debatable_statement)\n",
        "  print(\"debatable_statement by model1\")\n",
        "  print(debatable_statement)\n",
        "  debatable_statement = generate_debatable_statement(debatable_statement)\n",
        "  print(\"debatable_statement by model2\")\n",
        "  print(debatable_statement)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_debatable_statement_llama(initial_statement, model_name='facebook/llama-3'):\n",
        "    # Load pre-trained model tokenizer (vocabulary)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Encode the initial statement and add the EOS token\n",
        "    input_ids = tokenizer.encode(initial_statement + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # Load pre-trained model (weights)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    # Generate a response\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=100,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Decode the response\n",
        "    debatable_statement = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return debatable_statement\n",
        "debatable_statement = \"Climate change is the most significant issue facing the world.\"\n",
        "for i in range(10):\n",
        "  debatable_statement = generate_debatable_statement(debatable_statement)\n",
        "  print(\"debatable_statement by model1\")\n",
        "  print(debatable_statement)\n",
        "  debatable_statement = generate_debatable_statement(debatable_statement)\n",
        "  print(\"debatable_statement by model2\")\n",
        "  print(debatable_statement)\n"
      ],
      "metadata": {
        "id": "XmtuPXPhsFYx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}